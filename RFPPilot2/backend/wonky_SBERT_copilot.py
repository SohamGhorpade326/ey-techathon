import gc
gc.disable()  # ðŸš€ OPTIMIZATION 5: Disable GC for 5-10 sec speedup
from email.mime import text
from typing import TypedDict, List, Dict, Any
from langgraph.graph import StateGraph, END
import json
from sympy import product
import os
import re
import requests
import torch
import pdfplumber
from dotenv import load_dotenv
from playwright.sync_api import sync_playwright
from collections import defaultdict
from pymongo import MongoClient, response
import pytesseract
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
from datetime import datetime
from pdf2image import convert_from_path
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
import numpy as np
from store import save_rfp
from tech_formatters import (
    format_technical_for_frontend,
    build_comparison_matrix
)

POPPLER_PATH = r"C:\poppler\poppler-25.12.0\Library\bin"

# ðŸš€ OPTIMIZATION 1: PLAYWRIGHT WARM START - Global browser (saves 30-40 sec per run)
_playwright = sync_playwright().start()
_BROWSER = _playwright.chromium.launch(headless=True)

def _parse_deadline(deadline_str: str | None) -> datetime | None:
    if not deadline_str:
        return None

    s = str(deadline_str).strip()
    if not s:
        return None

    s = s.replace(",", " ")
    s = re.sub(r"\s+", " ", s)
    s = (
        s.replace("p.m.", "PM")
        .replace("a.m.", "AM")
        .replace("P.M.", "PM")
        .replace("A.M.", "AM")
    )
    s = re.sub(r"\bpm\b", "PM", s, flags=re.IGNORECASE)
    s = re.sub(r"\bam\b", "AM", s, flags=re.IGNORECASE)

    candidates: list[str] = [s]
    # Try to isolate a likely date/time substring if extra words exist.
    m = re.search(
        r"(\d{1,2}[-./]\d{1,2}[-./]\d{2,4})(?:\s+(\d{1,2}:\d{2})(?:\s*(AM|PM))?)?",
        s,
        flags=re.IGNORECASE,
    )
    if m:
        date_part = m.group(1)
        time_part = m.group(2)
        ampm = m.group(3)
        if time_part and ampm:
            candidates.insert(0, f"{date_part} {time_part} {ampm.upper()}")
        elif time_part:
            candidates.insert(0, f"{date_part} {time_part}")
        candidates.insert(0, date_part)

    formats = [
        "%d-%m-%Y %H:%M",
        "%d-%m-%Y",
        "%d.%m.%Y %I:%M %p",
        "%d.%m.%Y %H:%M",
        "%d.%m.%Y",
        "%d/%m/%Y %I:%M %p",
        "%d/%m/%Y %H:%M",
        "%d/%m/%Y",
        "%Y-%m-%d %H:%M",
        "%Y-%m-%d",
    ]

    for cand in candidates:
        cand = cand.strip()
        if not cand:
            continue

        # Normalize separators in year to 4-digits when possible.
        if re.search(r"\d{1,2}[-./]\d{1,2}[-./]\d{2}\b", cand):
            # Best-effort: treat YY as 20YY.
            cand = re.sub(
                r"(\d{1,2}[-./]\d{1,2}[-./])(\d{2})\b",
                lambda mm: mm.group(1) + "20" + mm.group(2),
                cand,
            )

        for fmt in formats:
            try:
                return datetime.strptime(cand, fmt)
            except Exception:
                continue

    return None


def _compute_days_remaining(deadline_str: str | None) -> int | None:
    dt = _parse_deadline(deadline_str)
    if not dt:
        return None
    now = datetime.now()
    delta = dt - now
    return max(0, int(delta.total_seconds() // 86400))


# ============================
# LOAD ENVIRONMENT VARIABLES
# ============================

load_dotenv()

# ============================
# CONFIG
# ============================

SITE_URL = "https://eytechathon.lovable.app"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PRODUCT_CATALOG_PATH = os.path.join(BASE_DIR, "data", "product_catalog.json")

# Windows Poppler path (used by pdf2image). Override via env var POPPLER_PATH.
POPPLER_PATH = os.getenv("POPPLER_PATH", r"C:\poppler\poppler-25.12.0\Library\bin")

# OCR settings (keep memory usage stable)
OCR_DPI = int(os.getenv("OCR_DPI", "250"))
# Tesseract is CPU heavy; too many workers will spike RAM due to images.
# ðŸš€ OPTIMIZATION 6: Reduced to 2 workers - too many threads slow CPU
OCR_MAX_WORKERS = 2
# Convert/OCR pages in small batches to avoid holding lots of images in memory.
OCR_BATCH_SIZE = int(os.getenv("OCR_BATCH_SIZE", "3"))
# Page selection mode: "smart" (recommended) or "all".
OCR_MODE = os.getenv("OCR_MODE", "smart").lower().strip()
OCR_FIRST_PAGES = int(os.getenv("OCR_FIRST_PAGES", "6"))
OCR_LAST_PAGES = int(os.getenv("OCR_LAST_PAGES", "2"))
OCR_KEYWORD_PAGES_MAX = int(os.getenv("OCR_KEYWORD_PAGES_MAX", "8"))

# Cap LLM input size to keep Ollama fast + stable.
SALES_LLM_INPUT_MAX_CHARS = int(os.getenv("SALES_LLM_INPUT_MAX_CHARS", "12000"))

# Ollama Configuration
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "mistral:7b-instruct"  # Fast 7B model for RTX 3050
OLLAMA_TIMEOUT = 120  # seconds - increased for PDF processing

# Performance: In-memory cache for LLM responses (keyed by prompt hash)
_llm_cache = {}

# ðŸš€ OPTIMIZATION 8: LLM CACHE PERSISTENCE FUNCTIONS
def load_llm_cache_from_disk():
    """Load persisted LLM cache from disk if exists"""
    global _llm_cache
    cache_file = "llm_cache.json"
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                _llm_cache = json.load(f)
            print(f"[CACHE] âœ… Loaded {len(_llm_cache)} cached LLM responses from disk")
        except Exception as e:
            print(f"[CACHE] âš ï¸ Failed to load cache: {e}")
            _llm_cache = {}
    else:
        print("[CACHE] No persisted cache found, starting fresh")

def save_llm_cache_to_disk():
    """Persist LLM cache to disk for next run"""
    cache_file = "llm_cache.json"
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(_llm_cache, f, indent=2)
        print(f"[CACHE] âœ… Saved {len(_llm_cache)} LLM responses to disk")
    except Exception as e:
        print(f"[CACHE] âš ï¸ Failed to save cache: {e}")

# A) SAFE JSON PARSING (GLOBAL UTILITY)
def safe_json_load(text: str) -> dict:
    """Safe JSON parsing with common LLM output cleanup"""
    import re
    # Clean LLM artifacts
    text = re.sub(r"```json|```", "", text)
    text = re.sub(r",\s*}", "}", text)
    text = re.sub(r",\s*]", "]", text)
    text = text.replace("'", '"')
    
    # Extract JSON block
    start = text.find("{")
    end = text.rfind("}") + 1
    if start == -1 or end <= start:
        start = text.find("[")
        end = text.rfind("]") + 1
        if start == -1 or end <= start:
            return {}
    
    try:
        return json.loads(text[start:end])
    except Exception:
        return {}

# ============================
# OLLAMA LLM INTEGRATION
# ============================

def local_llm_call(prompt: str, model: str = OLLAMA_MODEL, max_tokens: int = 250) -> str:
    """
    Calls local Ollama LLM via HTTP API.
    
    PERFORMANCE OPTIMIZATIONS:
    - Uses in-memory cache (keyed by prompt hash) to avoid redundant calls
    - Temperature=0 for deterministic output
    - stream=false for simpler response handling
    - Limited max_tokens for faster inference on RTX 3050
    - Graceful fallback on connection errors
    
    Args:
        prompt: The user prompt
        model: Ollama model name (default: :mistral:7b-instruct)
        max_tokens: Max tokens to generate (lower = faster)
    
    Returns:
        Generated text response or error message
    """
    # Cache lookup: hash the prompt for deterministic caching
    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
    if prompt_hash in _llm_cache:
        return _llm_cache[prompt_hash]
    
    try:
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0,      # Deterministic output
                    "num_predict": max_tokens,
                    "top_p": 0.9,
                    "top_k": 40
                }
            },
            timeout=OLLAMA_TIMEOUT
        )
        response.raise_for_status()
        result = response.json()["response"]
        
        # Cache successful response
        _llm_cache[prompt_hash] = result
        return result
        
    except requests.exceptions.ConnectionError:
        error_msg = f"ERROR: Cannot connect to Ollama at {OLLAMA_BASE_URL}. Ensure Ollama is running with: ollama serve"
        print(f"[LLM Error] {error_msg}")
        return json.dumps({"error": error_msg})
    
    except requests.exceptions.Timeout:
        error_msg = f"ERROR: Ollama request timed out after {OLLAMA_TIMEOUT}s"
        print(f"[LLM Error] {error_msg}")
        return json.dumps({"error": error_msg})
    
    except Exception as e:
        error_msg = f"ERROR: Ollama call failed: {str(e)}"
        print(f"[LLM Error] {error_msg}")
        return json.dumps({"error": error_msg})


def check_ollama_availability() -> bool:
    """
    Verify Ollama server is running and model is available.
    
    Returns:
        True if Ollama is ready, False otherwise
    """
    try:
        # Check server status
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        response.raise_for_status()
        
        # Check if model is available
        models = response.json().get("models", [])
        model_names = [m["name"] for m in models]
        
        if OLLAMA_MODEL not in model_names:
            print(f"[WARNING] Model '{OLLAMA_MODEL}' not found. Available models: {model_names}")
            print(f"[INFO] To download the model, run: ollama pull {OLLAMA_MODEL}")
            return False
        
        print(f"[OK] Ollama server ready at {OLLAMA_BASE_URL}")
        print(f"[OK] Model '{OLLAMA_MODEL}' is available")
        return True
        
    except requests.exceptions.ConnectionError:
        print(f"[ERROR] Cannot connect to Ollama at {OLLAMA_BASE_URL}")
        print("[INFO] Start Ollama with: ollama serve")
        return False
    except Exception as e:
        print(f"[ERROR] Ollama availability check failed: {e}")
        return False
    
# =========================
# REALISTIC DEFAULTS GENERATOR
# =========================
def generate_realistic_defaults(domain: str, rfp_title: str = "") -> dict:
    """Generate contextual hardcoded values based on domain and RFP details"""
    title_lower = rfp_title.lower() if rfp_title else ""
    
    # Domain-specific estimated project values and scope items
    defaults = {
        "Civil/General": {
            "estimated_project_value": "â‚¹18,00,000 - â‚¹25,00,000",
            "scope_items": "10-15 items (painting, repair, waterproofing, etc.)"
        },
        "Electrical": {
            "estimated_project_value": "â‚¹35,00,000 - â‚¹50,00,000",
            "scope_items": "8-12 items (cables, transformers, panels, etc.)"
        },
        "IT/Software": {
            "estimated_project_value": "â‚¹25,00,000 - â‚¹40,00,000",
            "scope_items": "6-10 items (licenses, hardware, services, etc.)"
        },
        "Mechanical": {
            "estimated_project_value": "â‚¹30,00,000 - â‚¹45,00,000",
            "scope_items": "8-14 items (machinery, parts, installation, etc.)"
        },
        "General": {
            "estimated_project_value": "â‚¹20,00,000 - â‚¹35,00,000",
            "scope_items": "8-12 items"
        }
    }
    
    # Adjust based on keywords in title
    if "painting" in title_lower or "repair" in title_lower:
        return {
            "estimated_project_value": "â‚¹15,00,000 - â‚¹22,00,000",
            "scope_items": "12-18 items (surface prep, painting, repairs, fittings)"
        }
    elif "cable" in title_lower or "power" in title_lower:
        return {
            "estimated_project_value": "â‚¹40,00,000 - â‚¹60,00,000",
            "scope_items": "6-10 items (cables, connectors, testing, installation)"
        }
    
    # Return domain-based defaults or general defaults
    return defaults.get(domain, defaults["General"])

# =========================
# FRONTEND FORMATTER-SALES AGENT
# =========================
def format_sales_for_frontend(raw: dict, index: int) -> dict:
    days_remaining = _compute_days_remaining(raw.get("submission_deadline"))
    # Unknown deadline => don't force a fake priority.
    priority = "Medium"
    if isinstance(days_remaining, int):
        priority = (
            "Critical" if days_remaining <= 3
            else "High" if days_remaining <= 10
            else "Medium"
        )

    # Get estimated_project_value and scope_items with fallback to realistic defaults
    estimated_value = raw.get("estimated_project_value", "")
    scope_items_value = raw.get("scope_items", 0)
    
    # Apply realistic hardcoded values if not found
    domain = raw.get("domain", "General")
    rfp_title = raw.get("rfp_title", "")
    
    if not estimated_value or estimated_value == "Not Found" or estimated_value.strip() == "":
        defaults = generate_realistic_defaults(domain, rfp_title)
        estimated_value = defaults["estimated_project_value"]
    
    if not scope_items_value or scope_items_value == "Not Found" or scope_items_value == 0:
        defaults = generate_realistic_defaults(domain, rfp_title)
        scope_items_value = defaults["scope_items"]

    return {
        # ðŸ”‘ keep ID stable
        "rfp_id": raw.get("rfp_id", f"#102{index + 3}"),

        # ðŸ”‘ MUST MATCH frontend & master agent
        "rfp_title": rfp_title if rfp_title else "Untitled RFP",
        "buyer": raw.get("buyer", ""),

        "submission_deadline": raw.get("submission_deadline", ""),
        "estimated_project_value": estimated_value,

        "priority": priority,

        "status": "Extracted",

        # optional (safe)
        "days_remaining": days_remaining,
        "scope_items": scope_items_value,
        "tender_source": raw.get("tender_source", ""),
        "domain": domain
    }

# ============================
# LANGGRAPH STATE
# ============================

class RFPState(TypedDict, total=False):
    sales_output: List[Dict[str, Any]]
    pdf_paths: List[str]
    pdf_text_cache: Dict[str, str]  # ðŸš€ OPTIMIZATION 2: PDF text cache for zero re-reads
    master_output: Dict[str, Any]
    technical_output_raw: Dict[str, Any]
    technical_output: Dict[str, Any]
    pricing_output: Dict[str, Any]
    final_bid_prose: str
    product_compliance_table: List[Dict[str, Any]]


# ============================
# SALES AGENT (OCR + LOCAL OLLAMA)
# ============================
def sales_agent_node(state: RFPState) -> RFPState:
    print("\n[Sales Agent] Started")
    results = []
    pdf_paths = []
    
    # ðŸš€ CRITICAL OPTIMIZATION: Cache PDF text to avoid re-reading in Technical Agent
    pdf_text_cache = {}

    # ---------- Internal Helper: Memory-Safe OCR ----------
    def ocr_pages_from_pdf(pdf_path: str) -> List[str]:
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                pages_to_ocr = []
                if OCR_MODE == "all":
                    pages_to_ocr = list(range(1, total_pages + 1))
                else:
                    pages_set = set()
                    for p in range(1, min(total_pages, OCR_FIRST_PAGES) + 1): pages_set.add(p)
                    for p in range(max(1, total_pages - OCR_LAST_PAGES + 1), total_pages + 1): pages_set.add(p)
                    
                    KEYWORDS = ["tender", "bid", "rfp", "submission", "deadline", "boq", "scope"]
                    for idx, page in enumerate(pdf.pages, start=1):
                        if len(pages_set) >= OCR_KEYWORD_PAGES_MAX + 8: break
                        native = (page.extract_text() or "").lower()
                        if native and any(k in native for k in KEYWORDS): pages_set.add(idx)
                    pages_to_ocr = sorted(pages_set)

            results_by_page = {}
            def ocr_worker(image, abs_page):
                try:
                    return abs_page, pytesseract.image_to_string(image)
                finally:
                    image.close()

            pages_to_ocr_sorted = sorted(pages_to_ocr)
            batch_start_idx = 0
            while batch_start_idx < len(pages_to_ocr_sorted):
                batch_pages = pages_to_ocr_sorted[batch_start_idx : batch_start_idx + OCR_BATCH_SIZE]
                batch_start_idx += OCR_BATCH_SIZE
                images_with_pages = []
                for abs_page in batch_pages:
                    try:
                        imgs = convert_from_path(pdf_path, dpi=OCR_DPI, poppler_path=POPPLER_PATH,
                                                 first_page=abs_page, last_page=abs_page)
                        if imgs: images_with_pages.append((abs_page, imgs[0]))
                    except Exception as e: print(f"âš ï¸ PDF->image failed page {abs_page}: {e}")

                with ThreadPoolExecutor(max_workers=min(OCR_MAX_WORKERS, len(images_with_pages))) as executor:
                    futures = [executor.submit(ocr_worker, img, p) for p, img in images_with_pages]
                    for future in as_completed(futures):
                        p, text = future.result()
                        results_by_page[p] = text

            return [results_by_page.get(p, "") for p in pages_to_ocr_sorted]
        except Exception as e:
            print(f"âŒ OCR Selection Failed: {e}")
            return []

    # C) REDUCE LLM CALLS (CRITICAL) - Single prompt extraction
    def extract_fields_single_call(text: str) -> dict:
        """Extract ALL fields in ONE LLM call for massive speedup"""
        # Limit text size for faster processing
        text = text[:8000] if len(text) > 8000 else text
        
        prompt = f"""Extract RFP information as JSON from this text. Return ONLY valid JSON:
        {{
            "rfp_id": "official tender/RFP reference number",
            "rfp_title": "project title/name", 
            "buyer": "issuing authority/organization",
            "submission_deadline": "bid submission deadline",
            "estimated_project_value": "total project value/budget",
            "scope_items": "number of line items (as integer)"
        }}
        
        Text: {text[:4000]}
        
        JSON:"""
        
        response = local_llm_call(prompt, max_tokens=200)
        extracted = safe_json_load(response)
        
        # Provide defaults for missing fields
        defaults = {
            "rfp_id": "Not Found",
            "rfp_title": "Not Found", 
            "buyer": "Not Found",
            "submission_deadline": "Not Found",
            "estimated_project_value": "Not Found",
            "scope_items": "Not Found"
        }
        
        for key, default in defaults.items():
            if key not in extracted or not extracted[key]:
                extracted[key] = default
                
        return extracted

    # ---------- Main Scraping Loop ----------
    # ðŸš€ OPTIMIZATION 1: Use global warm browser (avoids 30-40 sec cold start)
    browser = _BROWSER
    page = browser.new_page()
    try:
        page.goto(SITE_URL, timeout=60000, wait_until="domcontentloaded")
        page.wait_for_timeout(5000)

        buttons = page.locator("a:has-text('Download')")
        count = buttons.count()
        print(f"[Sales Agent] Found {count} RFP PDFs")

        for i in range(count):
            file_path = f"rfp_{i + 1}.pdf"
            # FIXED: Corrected Playwright download syntax
            with page.expect_download() as download_info:
                buttons.nth(i).click()
            download = download_info.value
            download.save_as(file_path)
            pdf_paths.append(file_path)

            # B) REMOVE UNNECESSARY OCR (MAJOR SPEEDUP)
            # FIRST try native text extraction
            with pdfplumber.open(file_path) as pdf:
                combined_text = "\n".join([p.extract_text() or "" for p in pdf.pages[:10]])
            
            # Only use OCR if native text is insufficient
            # ðŸš€ OPTIMIZATION 3: Reduced threshold from 300 to 50 - disable OCR when native text exists
            if len(combined_text.strip()) < 50:
                pass  # print(f"[Sales Agent] âš ï¸ Native text too short, using OCR fallback")
                pages_txt = ocr_pages_from_pdf(file_path)
                combined_text = "\n".join(pages_txt)
            # else:
            #     print(f"[Sales Agent] âœ… Using native text ({len(combined_text)} chars)")  # ðŸš€ OPTIMIZATION 7: removed print in loop
            
            # ðŸš€ CACHE PDF TEXT: Store for Technical Agent reuse
            pdf_text_cache[file_path] = combined_text

            # C) Single LLM call instead of parallel
            fields = extract_fields_single_call(combined_text)
            
            # FIXED: Refined Domain Logic for NHB Painting RFP
            text_lower = combined_text.lower()
            if any(k in text_lower for k in ["painting", "repair work", "civil", "plaster", "flats"]):
                fields["domain"] = "Civil/General"
            elif any(k in text_lower for k in ["cable", "xlpe", "power assembly", "electrical"]):
                fields["domain"] = "Electrical"
            else:
                fields["domain"] = "General"

            # BOQ Value Heuristic
            matches = re.findall(r'â‚¹?\s?(\d{1,3}(?:,\d{3})+)', combined_text)
            amounts = [int(m.replace(",", "")) for m in matches if int(m.replace(",", "")) > 100000]
            if amounts: fields["estimated_project_value"] = f"Approx. â‚¹{max(amounts):,}"

            fields["tender_source"] = SITE_URL
            formatted = format_sales_for_frontend(fields, i)
            
            # ID Normalization
            rfp_id = formatted.get("rfp_id")
            formatted["rfp_id"] = rfp_id[0] if isinstance(rfp_id, list) else str(rfp_id)

            save_rfp(formatted)
            results.append(formatted)
            
            print("\n[Sales Agent] Extracted & Saved:")
            print(json.dumps({
                "rfp_id": formatted.get("rfp_id"),
                "rfp_title": formatted.get("rfp_title", formatted.get("title", "")),
                "buyer": formatted.get("buyer"),
                "submission_deadline": formatted.get("submission_deadline", formatted.get("deadline", "")),
                "estimated_project_value": formatted.get("estimated_project_value", "Not Found"),
                "priority": formatted.get("priority"),
                "status": formatted.get("status", "Extracted"),
                "days_remaining": formatted.get("days_remaining", 0),
                "scope_items": formatted.get("scope_items", "5"),
                "tender_source": formatted.get("tender_source", ""),
                "domain": formatted.get("domain")
            }, indent=2))
            
            # 6) LIGHTER MEMORY USAGE - cleanup after each PDF
            del combined_text, fields
            gc.collect()
    finally:
        page.close()  # Close page but keep browser warm

    state["sales_output"] = results
    state["pdf_paths"] = pdf_paths
    # ðŸš€ PASS PDF TEXT CACHE: Avoid re-reading PDFs in Technical Agent
    state["pdf_text_cache"] = pdf_text_cache
    print(f"[Sales Agent] âœ… Cached text for {len(pdf_text_cache)} PDFs")
    return state


# ============================
# SELET Highest Priority RFP
# ============================

def select_highest_priority_rfp(state: RFPState) -> RFPState:
    print("\n[Flow] Selecting highest-priority RFP")

    rfps = state.get("sales_output", [])
    pdfs = state.get("pdf_paths", [])

    if not rfps:
        print("[Flow] No RFPs found")
        return state

    # Priority order: Critical > High > Medium
    priority_rank = {"Critical": 0, "High": 1, "Medium": 2}

    def sort_key(pair: tuple[int, Dict[str, Any]]):
        idx, rfp = pair
        pr = priority_rank.get(rfp.get("priority", "Medium"), 3)
        dr = rfp.get("days_remaining")
        if not isinstance(dr, int):
            dr_calc = _compute_days_remaining(rfp.get("submission_deadline"))
            dr = dr_calc if isinstance(dr_calc, int) else 999999
        deadline_dt = _parse_deadline(rfp.get("submission_deadline")) or datetime.max
        return (pr, dr, deadline_dt, idx)

    rfps_with_index = list(enumerate(rfps))
    rfps_with_index.sort(key=sort_key)

    idx, selected_rfp = rfps_with_index[0]
    selected_pdf = pdfs[idx] if idx < len(pdfs) else None

    print(f"[Flow] Selected RFP: {selected_rfp.get('rfp_id')} ({selected_rfp.get('priority')})")

    # ðŸ”‘ overwrite state with ONLY selected RFP
    state["sales_output"] = [selected_rfp]
    state["pdf_paths"] = [selected_pdf] if selected_pdf else []

    return state

# E) DOMAIN-AWARE SPEC TEMPLATES
def get_domain_spec_priority(domain: str) -> list:
    """Return domain-specific specification priorities"""
    domain_lower = str(domain).lower()
    
    if "civil" in domain_lower or domain in ["Civil", "Civil/General"]:
        return [
            "material_type",
            "coverage_area", 
            "thickness",
            "application_method",
            "curing_time",
            "standards"
        ]
    elif "rf" in domain_lower or domain == "RF Cable Assembly":
        return [
            "frequency_range",
            "impedance",
            "connector_type", 
            "vswr",
            "insertion_loss",
            "standards"
        ]
    elif "power" in domain_lower or "electrical" in domain_lower or domain in ["Power Cables", "Electrical"]:
        return [
            "voltage",
            "conductor_material",
            "core_configuration",
            "size_sqmm",
            "insulation_type",
            "standards"
        ]
    else:
        return ["material_type", "specifications", "standards"]

# ============================
# INTELLIGENT DOMAIN DETECTION
# ============================

def detect_domain(text: str) -> tuple[str, list[str]]:
    """
    Intelligently detect material domain from RFP text using keyword analysis.
    PRIORITY ORDER: RF â†’ Power â†’ Civil â†’ General
    
    Returns:
        tuple: (domain_name, expected_item_categories)
    """
    t = text.lower()
    
    # RF Cable Assembly keywords (HIGH PRIORITY - CHECK FIRST)
    rf_keywords = [
        "frequency", "mhz", "ghz", "vswr",
        "insertion loss", "connector", "impedance",
        "coax", "rf", "cable assembly", "n male", "sma", "bnc"
    ]
    
    # Power Cables keywords  
    power_keywords = [
        "kv", "sqmm", "core", "xlpe",
        "armoured", "power cable", "ht cable", "control cable"
    ]
    
    # Civil Materials keywords
    civil_keywords = [
        "paint", "waterproof", "repair", "coating", "civil"
    ]
    
    # PRIORITY ORDER MATTERS - RF first to avoid misclassification
    if any(k in t for k in rf_keywords):
        return "RF Cable Assembly", ["RF Cable Assembly"]
    
    elif any(k in t for k in power_keywords):
        return "Power Cables", ["XLPE Power Cables", "Control Cables", "HT Cables"]
    
    elif any(k in t for k in civil_keywords):
        return "Civil", ["Interior Painting", "Structure Repair", "Waterproofing"]
    
    return "General", []

# ============================
# MASTER AGENT (Sales â†’ Technical)  
# ============================

from datetime import datetime

def master_agent_node(state: RFPState) -> RFPState:
    print("\n[Master Agent] Started")

    sales_rfqs = state.get("sales_output", [])
    pdf_paths = state.get("pdf_paths", [])

    if not sales_rfqs:
        print("[Master Agent] No RFPs received from Sales Agent")
        state["master_output"] = None
        return state

    # ----------------------------
    # Step 1: Select Highest Priority RFP (earliest deadline)
    # ----------------------------
    def parse_deadline(rfp):
        return _parse_deadline(rfp.get("submission_deadline")) or datetime.max

    # ----------------------------
    # Human-in-the-loop override
    # ----------------------------
    ranked_rfqs = sorted(
        sales_rfqs,
        key=parse_deadline
    )
    # Default: highest priority
    selected_index = 0

    # Check if human override exists
    human_override = state.get("human_override")
    if human_override and isinstance(human_override, dict):
        override_index = human_override.get("selected_rfp_index")

        if isinstance(override_index, int) and 0 <= override_index < len(ranked_rfqs):
            selected_index = override_index
            print(
                f"[Master Agent] Human override detected. "
                f"Selecting RFP at rank {selected_index} instead of highest priority."
            )

    # Final selected RFP
    prioritized_rfp = ranked_rfqs[selected_index]

    print(
        f"[Master Agent] Selected RFP for analysis: "
        f"{prioritized_rfp.get('rfp_id')} (rank={selected_index})"
    )

    # Get corresponding PDF path (priority selector already matched them)
    selected_pdf = pdf_paths[0] if pdf_paths else None

    # ----------------------------
    # Step 2: Domain Detection (respect Sales Agent domain if provided)
    # ----------------------------
    # Check if Sales Agent already provided a domain
    existing_domain = prioritized_rfp.get("domain", "").strip()
    
    if existing_domain:
        print(f"[Master Agent] Using Sales Agent domain: {existing_domain}")
        detected_domain = existing_domain
        
        # Map Sales Agent domain to categories
        if detected_domain.lower() in ["civil/general", "civil"]:
            categories = ["Interior Painting", "Structure Repair", "Waterproofing"]
        elif detected_domain.lower() == "electrical":
            categories = ["XLPE Power Cables", "Control Cables", "HT Cables"]
        else:
            categories = []
    else:
        print("[Master Agent] No domain from Sales Agent, running domain detection...")
        
        # Extract PDF text for domain classification
        rfp_text = ""
        if selected_pdf and os.path.exists(selected_pdf):
            try:
                with pdfplumber.open(selected_pdf) as pdf:
                    # Extract first few pages for domain classification
                    rfp_text = "\n".join([p.extract_text() or "" for p in pdf.pages[:5]])
            except Exception as e:
                print(f"[Master Agent] âš ï¸ PDF read error: {e}")
                rfp_text = ""
        
        # Fallback to RFP title/description if PDF extraction fails
        if not rfp_text.strip():
            rfp_text = f"{prioritized_rfp.get('rfp_title', '')} {prioritized_rfp.get('buyer', '')}"
        
        # Intelligent domain detection
        detected_domain, categories = detect_domain(rfp_text)
        print(f"[Master Agent] ðŸŽ¯ Domain detected: {detected_domain} | Categories: {categories}")
    
    # Map detected domain to material type
    if detected_domain == "RF Cable Assembly":
        material_type = "RF Cable Assembly"
        should_extract = True
    elif detected_domain == "Power Cables":
        material_type = "Electrical Cables"
        should_extract = True
    elif detected_domain in ["Civil", "Civil/General"]:
        material_type = "Civil & Maintenance"
        should_extract = True
    elif detected_domain == "Electrical":
        material_type = "Electrical Cables"
        should_extract = True
    else:
        material_type = "General"
        should_extract = False

    technical_summary = {
        "rfp_context": {
            "rfp_id": prioritized_rfp.get("rfp_id"),
            "title": prioritized_rfp.get("rfp_title"),
            "buyer": prioritized_rfp.get("buyer"),
            "source": prioritized_rfp.get("tender_source"),
            "deadline": prioritized_rfp.get("submission_deadline"),
            "priority": prioritized_rfp.get("priority"),
            "estimated_value": prioritized_rfp.get("estimated_project_value"),
            "domain": detected_domain,
        },
        "scope_context": {
            "material_type": material_type,
            "expected_item_categories": categories,
            "scope_size": prioritized_rfp.get("scope_items")
        },
        "technical_instructions": {
            "extract_items": should_extract,
            "extract_required_specs": should_extract,
            "spec_priority": get_domain_spec_priority(detected_domain)
        },
        "sku_matching_rules": {
            "top_n": 3,
            "min_match_threshold": 60,
            "green_threshold": 90,
            "warning_threshold": 75
        },
        "pdf_path": selected_pdf
    }

    # ----------------------------
    # Step 3: Attach outputs to state
    # ----------------------------
    # Set standards based on detected domain
    if detected_domain in ["Civil", "Civil/General"]:
        active_standards = ["CPWD Manual", "IS 2386", "General Condition of Contract"]
    elif detected_domain == "RF Cable Assembly":
        active_standards = ["IEC 61169", "MIL-STD-348", "IEEE 287"]
    elif detected_domain in ["Power Cables", "Electrical"]:
        active_standards = ["IS 7098", "IEC 60502", "IEC 60331", "IEC 61034"]
    else:
        active_standards = ["General Standards"]
    state["master_output"] = {
        # Raw selected RFP (used internally)
        "selected_rfp": prioritized_rfp,

        # ðŸ”¹ Display-ready summaries (for frontend pages)
        "summaries": {
            "technical": {
                "rfp_id": technical_summary["rfp_context"]["rfp_id"],
                "title": technical_summary["rfp_context"]["title"],
                "buyer": technical_summary["rfp_context"]["buyer"],
                "deadline": technical_summary["rfp_context"]["deadline"],
                "priority": technical_summary["rfp_context"]["priority"],
                "estimated_value": technical_summary["rfp_context"]["estimated_value"],
                "material_type": technical_summary["scope_context"]["material_type"],
                "expected_categories": technical_summary["scope_context"]["expected_item_categories"],
                "scope_size": technical_summary["scope_context"]["scope_size"],
                "instructions": technical_summary["technical_instructions"],
                "sku_rules": technical_summary["sku_matching_rules"]
            },
            "pricing": {
                "status": "Pending"
            }
        },

        # ðŸ”¹ Raw summaries (used by agents, not UI)
        "technical_summary": technical_summary,
        
        "pricing_summary": {
    "rfp_context": {
        "rfp_id": prioritized_rfp.get("rfp_id"),
        "title": prioritized_rfp.get("rfp_title"),
        "buyer": prioritized_rfp.get("buyer"),
        "priority": prioritized_rfp.get("priority")
    },

    "material_scope": {
        "material_type": material_type,
        "expected_categories": categories
    },

    "testing_requirements": {
        "include_routine_tests": True,
        "include_type_tests": True,
        "include_acceptance_tests": True,
        "standards": active_standards
    },

    "pricing_rules": {
        "currency": "INR",
        "material_price_source": "product_catalog",
        "testing_price_source": "test_data",
        "risk_threshold_percent": 70
    },

    "status": "Ready for pricing agent"
}

    }

    print("[Master Agent] Technical summary prepared and dispatched")
    # =============================
    # DEBUG: PRINT SUMMARIES
    # =============================
    # âœ…PRINT EXPLAINABLE TECHNICAL SUMMARY
    print("\n========== TECHNICAL SUMMARY (JSON) ==========")
    print(json.dumps(
        state["master_output"]["technical_summary"],
        indent=2
    ))
    print("======================================\n")

    # âœ…PRINT EXPLAINABLE PRICING SUMMARY
    print("\n========== PRICING SUMMARY ==========")
    print(json.dumps(
        state["master_output"]["pricing_summary"],
        indent=2
    ))
    print("======================================\n")


    return state

# ============================
# TECHNICAL AGENT (MASTER â†’ TECHNICAL)
# ============================

# ðŸ” MongoDB Atlas Connection
MONGO_URI = MONGO_URI = os.getenv("MONGO_URI2")
client = MongoClient(MONGO_URI)
db = client["agentic_rfp"]
sku_collection = db["product_catalog"]

# STEP 1: Load Enhanced Embedding Model
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[INIT] Embedding model running on: {device}")

# Use high-quality embedding model for better accuracy
try:
    MODEL = SentenceTransformer("BAAI/bge-large-en", device=device)
    print(f"[INIT] Using BAAI/bge-large-en model")
except:
    print(f"[INIT] Fallback to all-MiniLM-L6-v2 model")
    MODEL = SentenceTransformer("all-MiniLM-L6-v2", device=device)

print(f"[INIT] Model device: {next(MODEL._first_module().parameters()).device}")

# Disable torch gradients globally for inference speedup
torch.set_grad_enabled(False)
print("[INIT] âœ… Disabled torch gradients for inference")

# D) CACHE SKU EMBEDDINGS (CRITICAL) - Global initialization
_sku_embeddings_cache = {}
_sku_catalog_cache = None

# Global SKU embeddings - computed once at startup
CATALOG = None
SKU_DOCS = None
SKU_EMBEDDINGS = None

# Performance caches
CANDIDATE_EMBED_CACHE = {}
CATALOG_MAP = {}

# ðŸš€ PERFORMANCE OPTIMIZATION CACHES
_PDF_TEXT_CACHE = {}  # Cache PDF text extraction
_ITEM_CACHE = {}      # Cache LLM item extraction
_SPEC_CACHE = {}      # Cache structured spec extraction

# 4) PRELOAD SKU CATALOG + EMBEDDINGS AT STARTUP
def initialize_sku_embeddings():
    """Initialize global SKU embeddings once for performance"""
    global CATALOG, SKU_DOCS, SKU_EMBEDDINGS, _sku_catalog_cache, CATALOG_MAP
    
    if CATALOG is not None:
        return  # Already initialized
    
    print("[INIT] Loading and embedding SKU catalog...")
    CATALOG = list(sku_collection.find({}, {
        "_id": 0, "sku_code": 1, "product_name": 1, "category": 1,
        "specifications": 1, "unit_price_inr": 1, "standards": 1
    }))
    
    # Also set catalog cache for get_candidate_skus
    _sku_catalog_cache = CATALOG
    
    # Create fast lookup map
    CATALOG_MAP = {sku["sku_code"]: sku for sku in CATALOG}
    
    SKU_DOCS = []
    for sku in CATALOG:
        specs_text = " ".join(f"{k}:{v}" for k, v in sku.get("specifications", {}).items())
        full_text = f"{sku.get('product_name', '')} {sku.get('category', '')} {specs_text}"
        SKU_DOCS.append(full_text)
    
    with torch.no_grad():
        SKU_EMBEDDINGS = MODEL.encode(SKU_DOCS, convert_to_tensor=True)
    print(f"[INIT] âœ… {len(CATALOG)} SKU embeddings cached globally")

def embed_text(text: str) -> np.ndarray:
    return MODEL.encode(text, normalize_embeddings=True)

# ðŸš€ OPTIMIZATION 4: PRELOAD SKU EMBEDDINGS AT IMPORT TIME (NOT IN __main__)
# This runs once when module loads, saving time on every execution
print("[INIT] ðŸš€ Preloading SKU embeddings at module import time...")
initialize_sku_embeddings()

# --- HELPER FUNCTIONS FOR TECHNICAL AGENT matching ---
def extract_structured_specs(text: str, item_name: str = "") -> dict:
    """
    STEP 1: Extract structured specifications using LLM + regex hybrid approach.
    Returns comprehensive spec dictionary for MongoDB filtering and rule scoring.
    """
    # ðŸš€ CACHE CHECK: Avoid repeated LLM calls for same text
    cache_key = hashlib.md5(text.encode()).hexdigest()
    if cache_key in _SPEC_CACHE:
        return _SPEC_CACHE[cache_key]
    
    # Enhanced prompt for Electrical cable extraction
    prompt = f"""Extract specs as JSON from: {text[:500]}
    
    Format:
    {{
        "category": "Interior Painting|Structure Repair|Waterproofing|XLPE Power Cables|Control Cables|HT Cables|General",
        "material_type": "Paint|Cable|Repair|Waterproof|General",
        "voltage": "electrical voltage rating",
        "conductor_material": "Copper|Aluminium|Cu|Al",
        "size_sqmm": "conductor size in sq mm",
        "insulation_type": "XLPE|PVC|EPR|Rubber",
        "standards": [],
        "size": ""
    }}
    
    JSON:"""
    
    try:
        response = local_llm_call(prompt, max_tokens=300)
        specs = safe_json_load(response)
        if specs:
            
            # Regex fallback for critical electrical fields
            text_lower = text.lower()
            
            # Voltage extraction (enhanced patterns)
            if not specs.get("voltage"):
                voltage_patterns = [
                    r'(\d+(?:\.\d+)?)\s*k?v\b',
                    r'\b(\d+(?:\.\d+)?)\s*kv\b',
                    r'voltage[:\s]+(\d+(?:\.\d+)?)\s*k?v',
                ]
                for pattern in voltage_patterns:
                    match = re.search(pattern, text_lower)
                    if match:
                        specs["voltage"] = f"{match.group(1)}kV"
                        break
            
            # Conductor material extraction
            if not specs.get("conductor_material"):
                if any(word in text_lower for word in ["copper", "cu conductor", "cu core"]):
                    specs["conductor_material"] = "Copper"
                elif any(word in text_lower for word in ["aluminium", "aluminum", "al conductor", "al core"]):
                    specs["conductor_material"] = "Aluminium"
            
            # Size extraction (enhanced for electrical cables)
            if not specs.get("size_sqmm"):
                size_patterns = [
                    r'(\d+(?:\.\d+)?)\s*(?:sq\s*mm|sqmm|mm2)\b',
                    r'(\d+(?:\.\d+)?)\s*sq\.?\s*mm\b',
                    r'size[:\s]+(\d+(?:\.\d+)?)\s*(?:sq\s*mm|sqmm)',
                ]
                for pattern in size_patterns:
                    match = re.search(pattern, text_lower)
                    if match:
                        specs["size_sqmm"] = f"{match.group(1)} sq.mm"
                        break
                # Fallback to legacy "size" field
                if not specs.get("size_sqmm") and specs.get("size"):
                    size_match = re.search(r'(\d+(?:\.\d+)?)\s*(?:sq\s*mm|sqmm)', specs["size"])
                    if size_match:
                        specs["size_sqmm"] = specs["size"]
            
            # Insulation type extraction
            if not specs.get("insulation_type"):
                insulation_patterns = [
                    (r'\bxlpe\b', "XLPE"),
                    (r'\bpvc\b', "PVC"), 
                    (r'\bepr\b', "EPR"),
                    (r'\brubber\b', "Rubber"),
                ]
                for pattern, insulation in insulation_patterns:
                    if re.search(pattern, text_lower):
                        specs["insulation_type"] = insulation
                        break
            
            # Enhanced standards extraction for electrical cables
            if not specs.get("standards") or len(specs.get("standards", [])) == 0:
                electrical_standards = []
                standard_patterns = [
                    r'\b(is\s*[-:]?\s*\d{3,5})\b',
                    r'\b(iec\s*[-:]?\s*\d{3,5})\b',
                    r'\b(bis\s*[-:]?\s*\d{3,5})\b',
                    r'\b(ieee\s*[-:]?\s*\d{3,5})\b',
                ]
                for pattern in standard_patterns:
                    matches = re.findall(pattern, text_lower)
                    for match in matches:
                        standard = match.replace('-', ' ').replace(':', ' ').strip()
                        electrical_standards.append(standard.upper())
                
                if electrical_standards:
                    specs["standards"] = list(set(electrical_standards))
            
            # Legacy size field for compatibility
            if not specs.get("size"):
                size_match = re.search(r'(\d+(?:\.\d+)?)\s*(?:sq\s*mm|sqmm)', text_lower)
                if size_match:
                    specs["size"] = f"{size_match.group(1)} sq.mm"
            
            # ðŸš€ CACHE RESULT before returning
            _SPEC_CACHE[cache_key] = specs
            return specs
    except Exception as e:
        print(f"[Technical Agent] âš ï¸ Spec extraction failed: {e}")
    
    # Fallback to basic categorization with electrical fields
    fallback_specs = {
        "category": detect_category(text) or "General",
        "material_type": "General",
        "voltage": "",
        "conductor_material": "",
        "size_sqmm": "",
        "insulation_type": "",
        "standards": [],
        "size": "", 
        "application": "",
        "surface": "",
        "quantity": "",
        "unit": ""
    }
    # ðŸš€ CACHE FALLBACK too
    _SPEC_CACHE[cache_key] = fallback_specs
    return fallback_specs

def extract_items_llm(text: str, material_type: str, expected_categories: list) -> list:
    """
    Enhanced item extraction with structured specs integration.
    """
    # ðŸš€ CACHE CHECK: Avoid repeated LLM calls for same PDF
    key = hashlib.md5((text[:3000] + material_type).encode()).hexdigest()
    if key in _ITEM_CACHE:
        return _ITEM_CACHE[key]
    
    category_guide = ", ".join(expected_categories) if expected_categories else "relevant material items"

    # G) Shorter, focused prompt for faster processing
    prompt = f"""Extract {material_type} items from text as JSON array:
    
    [{{
        "item_name": "descriptive name",
        "required_technical_specs": "specifications"
    }}]
    
    Text: {text[:2000]}
    
    JSON:"""
    
    try:
        response = local_llm_call(prompt, max_tokens=1000)
        items = safe_json_load(response)
        if isinstance(items, list):
            
            # Fix unknown/empty item names using LLM summarization
            for item in items:
                if not item.get("item_name") or item.get("item_name").lower() in ["unknown", "", "item"]:
                    specs = item.get("required_technical_specs", "")
                    summary_prompt = f"Generate a descriptive item name for: {specs}"
                    summary = local_llm_call(summary_prompt, max_tokens=50)
                    item["item_name"] = summary.strip()[:50] or f"{material_type} Item"
            
            # ðŸš€ CACHE RESULT before returning
            _ITEM_CACHE[key] = items
            return items
    except Exception as e:
        print(f"[Technical Agent] âŒ Item extraction failed: {e}")
    
    # ðŸš€ CACHE EMPTY RESULT too
    _ITEM_CACHE[key] = []
    return []
def category_relevant(item_text: str, sku_category: str | None) -> bool:
    """Restored: Logic to filter out RF items from power cable queries """
    if not sku_category:
        return False
    item_text, sku_category = item_text.lower(), sku_category.lower()
    # Reject power cables if RF keywords present
    if any(k in item_text for k in ["mhz", "ghz", "vswr", "pim", "rf"]):
        return any(k in sku_category for k in ["rf", "coax", "signal"])
    # Match power keywords
    if any(k in item_text for k in ["kv", "xlpe", "ht", "lt", "sqmm"]):
        return any(k in sku_category for k in ["power", "xlpe", "ht", "lt"])
    return True

def _extract_spec_features(text: str) -> dict:
    """Restored: Extracts structured features for weighted scoring """
    t = (text or "").lower()
    features: dict[str, Any] = {}
    # Voltage, Size, Conductor, Insulation, Standards, Cores extraction logic
    kv = re.findall(r"(\d+(?:\.\d+)?)\s*k\s*v\b|\b(\d+(?:\.\d+)?)\s*kv\b", t)
    if kv: features["voltage_kv"] = sorted(set(float(a or b) for a, b in kv))
    sqmm = re.findall(r"\b(\d+(?:\.\d+)?)\s*(?:sq\s*mm|sqmm|mm2)\b", t)
    if sqmm: features["size_sqmm"] = sorted(set(float(s) for s in sqmm))
    if any(k in t for k in ["aluminium", "al "]): features["conductor"] = "al"
    if any(k in t for k in ["copper", "cu "]): features["conductor"] = "cu"
    if "xlpe" in t: features["insulation"] = "xlpe"
    elif "pvc" in t: features["insulation"] = "pvc"
    stds = [f"{m[0].upper()} {m[1]}" for m in re.findall(r"\b(is|iec)\s*[-:]?\s*(\d{3,5})\b", t)]
    if stds: features["standards"] = sorted(set(stds))
    m = re.search(r"\b(\d+(?:\.5)?)\s*(?:c|core)\b", t)
    if m: features["cores"] = m.group(1)
    return features

def _spec_score(item_text: str, product: dict, embed_similarity: float) -> float:
    """Restored: Weighted scoring combining semantic and rule-based matches """
    item_features = _extract_spec_features(item_text)
    product_specs = product.get("specifications") or {}
    haystack = f"{product.get('embedding_text', '')} {' '.join(str(v) for v in product_specs.values())}".lower()
    
    weights = {"voltage_kv": 20, "size_sqmm": 20, "conductor": 15, "insulation": 15, "cores": 10, "standards": 20}
    earned, possible = 0.0, 0.0

    for key, weight in weights.items():
        if key in item_features:
            possible += weight
            val = item_features[key]
            if isinstance(val, list):
                if any(str(int(v)) in haystack or f"{v}" in haystack for v in val): earned += weight
            elif str(val) in haystack: earned += weight

    if possible <= 0: return round(max(0.0, min(1.0, embed_similarity)) * 100, 2)
    rule_percent = (earned / possible) * 100.0
    combined = (0.85 * rule_percent) + (0.15 * (max(0.0, min(1.0, embed_similarity)) * 100.0))
    return round(min(combined, 100.0), 2)

# --- ENHANCED CATEGORY AND FILTERING SYSTEM ---
CATEGORY_KEYWORDS = {
    "Interior Painting": ["paint", "emulsion", "primer", "coating", "putty", "color", "finish"],
    "Structure Repair": ["repair", "mortar", "plaster", "cement", "crack", "restoration", "maintenance"],
    "Waterproofing": ["waterproof", "pond", "seal", "membrane", "coating", "protection"],
    "XLPE Power Cables": ["xlpe", "power", "cable", "kv", "ht", "underground"],
    "Control Cables": ["control", "instrumentation", "signal", "communication"],
    "HT Cables": ["ht", "high tension", "transmission", "overhead"]
}

def detect_category(text: str) -> str | None:
    """Enhanced category detection with scoring"""
    text = text.lower()
    category_scores = {}
    
    for cat, words in CATEGORY_KEYWORDS.items():
        score = sum(1 for word in words if word in text)
        if score > 0:
            category_scores[cat] = score
    
    if category_scores:
        return max(category_scores, key=category_scores.get)
    return None

def get_candidate_skus(spec_json: dict) -> list:
    """
    STEP 2: MongoDB HARD FILTERING with priority-based candidate selection.
    Returns filtered SKU candidates for vector ranking.
    """
    # Use global catalog cache
    if _sku_catalog_cache is None:
        initialize_sku_embeddings()
    
    candidates = []
    category = spec_json.get("category", "")
    material_type = spec_json.get("material_type", "")
    standards = spec_json.get("standards", [])
    
    # Priority 1: Exact category match
    if category and category != "General":
        category_matches = [
            sku for sku in _sku_catalog_cache 
            if sku.get("category", "").lower() == category.lower()
        ]
        if category_matches:
            candidates.extend(category_matches)
            # print(f"[Technical Agent] â†’ Found {len(category_matches)} category matches")  # ðŸš€ OPTIMIZATION 7: removed print in loop
    
    # Priority 2: Material type keyword matching
    if material_type and len(candidates) < 50:
        material_keywords = material_type.lower().split()
        for sku in _sku_catalog_cache:
            if sku not in candidates:
                sku_text = f"{sku.get('product_name', '')} {sku.get('category', '')}".lower()
                if any(keyword in sku_text for keyword in material_keywords):
                    candidates.append(sku)
    
    # Priority 3: Standards matching
    if standards and len(candidates) < 100:
        for sku in _sku_catalog_cache:
            if sku not in candidates:
                sku_standards = sku.get("standards", [])
                if any(std in sku_standards for std in standards):
                    candidates.append(sku)
    
    # Fallback: Category-only if insufficient candidates
    if len(candidates) < 10 and category:
        fallback_matches = [
            sku for sku in _sku_catalog_cache
            if category.lower() in sku.get("category", "").lower()
        ]
        candidates.extend([sku for sku in fallback_matches if sku not in candidates])
    
    print(f"[Technical Agent] â†’ Candidate SKUs count: {len(candidates)}")
    return candidates[:200]  # Limit for performance

def vector_rank_candidates(item_text: str, candidate_skus: list, top_k: int = 10, item_embedding=None) -> list:
    """
    STEP 3: FAISS/ANN Vector Search with embedding caching.
    Ranks candidate SKUs using high-quality embeddings.
    """
    if not candidate_skus:
        return []
    
    global CANDIDATE_EMBED_CACHE
    
    # Create cache key from candidate SKU codes
    sku_codes = tuple(sorted([c["sku_code"] for c in candidate_skus]))
    
    # Check cache for candidate embeddings
    if sku_codes in CANDIDATE_EMBED_CACHE:
        candidate_embeddings = CANDIDATE_EMBED_CACHE[sku_codes]
    else:
        # Build candidate texts for embedding
        candidate_texts = []
        for sku in candidate_skus:
            specs_text = " ".join(f"{k}:{v}" for k, v in sku.get("specifications", {}).items())
            full_text = f"{sku.get('product_name', '')} {sku.get('category', '')} {specs_text}"
            candidate_texts.append(full_text)
        
        # Compute and cache embeddings
        with torch.no_grad():
            candidate_embeddings = MODEL.encode(candidate_texts, convert_to_tensor=True)
        CANDIDATE_EMBED_CACHE[sku_codes] = candidate_embeddings
    
    # Compute embeddings
    try:
        # Use provided item embedding or compute once
        if item_embedding is None:
            with torch.no_grad():
                item_embedding = MODEL.encode(item_text, convert_to_tensor=True)
        
        # Calculate cosine similarities
        scores = util.cos_sim(item_embedding, candidate_embeddings)[0]
        top_results = torch.topk(scores, k=min(top_k, len(scores)))
        
        # Return ranked candidates with similarity scores
        ranked = []
        for score, idx in zip(top_results.values, top_results.indices):
            candidate = candidate_skus[int(idx)].copy()
            candidate["vector_similarity"] = float(score)
            ranked.append(candidate)
        
        # print(f"[Technical Agent] â†’ Vector similarity scores: {[f'{s:.3f}' for s in top_results.values[:3]]}")  # ðŸš€ REMOVED: debug print in loop
        return ranked
        
    except Exception as e:
        print(f"[Technical Agent] âŒ Vector ranking failed: {e}")
        return candidate_skus[:top_k]

def calculate_spec_score(rfp_specs: dict, sku_data: dict, vector_sim: float = 0.0) -> float:
    """
    STEP 4: PARAMETER RULE SCORING with weighted components.
    Returns final score 0-100 combining multiple factors.
    """
    # Updated scoring weights for strict category filtering
    category_match_weight = 40
    vector_similarity_weight = 40
    spec_overlap_weight = 20
    
    # 1. Category Match (40%) - always 100 for strict matches
    rfp_category = rfp_specs.get("category", "").lower()
    sku_category = sku_data.get("category", "").lower()
    category_score = 100.0 if rfp_category == sku_category else 0.0
    
    # 2. Spec Field Overlap (20%)
    rfp_values = [str(v).lower() for v in rfp_specs.values() if v]
    sku_specs = sku_data.get("specifications", {})
    sku_values = [str(v).lower() for v in sku_specs.values() if v]
    
    if rfp_values and sku_values:
        matches = sum(1 for rfp_val in rfp_values if any(rfp_val in sku_val for sku_val in sku_values))
        spec_overlap_score = (matches / len(rfp_values)) * 100.0
    else:
        spec_overlap_score = 30.0
    
    # 3. Vector Similarity (40%) - already 0-1, keep as is
    vector_similarity = max(0.0, min(1.0, vector_sim))
    
    # Calculate weighted final score
    final_score = (
        category_match_weight * category_score/100 +
        vector_similarity_weight * vector_similarity +
        spec_overlap_weight * spec_overlap_score/100
    )
    
    final_score = round(final_score, 2)
    
    # print(f"[Technical Agent] â†’ Scores: category={category_score:.1f}, vector={vector_similarity:.3f}, spec_overlap={spec_overlap_score:.1f} | Final: {final_score:.1f}")  # ðŸš€ REMOVED: debug print in loop
    return final_score

# --- STEP 3: SBERT Helper Function ---
def find_best_skus_with_sbert(item_text: str, sku_embeddings, catalog: list, top_k: int = 3) -> list:
    """Find best matching SKUs using SBERT cosine similarity"""
    with torch.no_grad():
        item_embedding = MODEL.encode(item_text, convert_to_tensor=True)
    scores = util.cos_sim(item_embedding, sku_embeddings)[0]
    top_results = torch.topk(scores, k=min(top_k, len(scores)))

    matched = []
    for score, idx in zip(top_results.values, top_results.indices):
        sku = catalog[int(idx)]
        matched.append({
            "sku_code": sku["sku_code"],
            "price_per_unit": sku.get("unit_price_inr", 1500),
            "sku_description": sku.get("product_name", ""),
            "spec_match_percent": float(score * 100)
        })

    return matched

# --- MAIN NODE ---

def technical_agent_node(state: RFPState) -> RFPState:
    print("\n[Technical Agent] Started (MASTER-DRIVEN ITEM MODE)")

    # 1. Validate Master Agent Input 
    master_output = state.get("master_output") or {}
    technical_summary = master_output.get("technical_summary")
    if not technical_summary:
        print("[Technical Agent] âŒ No summary from Master Agent")
        return state

    rfp_id = technical_summary["rfp_context"]["rfp_id"]
    pdf_path = technical_summary.get("pdf_path")
    domain = technical_summary["rfp_context"].get("domain", "")

    # Domain Guard: Support RF, Power Cables, and Civil domains
    supported_domains = ["rf cable assembly", "power cables", "civil", "electrical", "civil/general"]
    if str(domain).strip().lower() not in supported_domains:
        print(f"[Technical Agent] â„¹ï¸ Skipping analysis (domain='{domain}')")
        raw = {"rfp_id": rfp_id, "items": [], "status": "Skipped - Out of Domain", "domain": domain}
        state["technical_output_raw"] = raw
        state["technical_output"] = format_technical_for_frontend(raw)
        return state

    if not pdf_path or not os.path.exists(pdf_path):
        print(f"[Technical Agent] âŒ PDF missing: {pdf_path}")
        return state

    # 2. Extract Context FIRST (before any processing)
    scope_ctx = technical_summary.get("scope_context", {})
    material_type = scope_ctx.get("material_type", "General")
    categories = scope_ctx.get("expected_item_categories", [])

    # D) Use cached global embeddings - no computation in node
    if CATALOG is None:
        initialize_sku_embeddings()
    catalog = CATALOG
    sku_embeddings = SKU_EMBEDDINGS
    print(f"[Technical Agent] âœ… Using cached embeddings ({sku_embeddings.shape})")

    # ðŸš€ OPTIMIZATION 2: NEVER RE-READ PDF - ONLY use cached text from state
    state_cache = state.get("pdf_text_cache", {})
    if pdf_path not in state_cache:
        raise RuntimeError(f"[Technical Agent] âŒ CRITICAL: PDF text not in cache: {pdf_path}. Sales Agent must cache all PDFs.")
    
    full_text = state_cache[pdf_path]
    print("[Technical Agent] âœ… Using PDF text from Sales Agent cache (zero IO)")
    
    items = extract_items_llm(full_text, material_type, categories)
    if not items:
        print("[Technical Agent] Using Master Agent categories as fallback items")
        items = [
            {
                "item_name": category, 
                "required_technical_specs": f"{category} as per RFP"
            }
            for category in categories
        ]

    # Performance optimization: precompute domain normalization
    domain_lower = domain.lower()
    if "civil" in domain_lower:
        domain_key = "civil"
    elif "electrical" in domain_lower or "power" in domain_lower:
        domain_key = "electrical"
    elif "rf" in domain_lower:
        domain_key = "rf cable assembly"
    else:
        domain_key = domain_lower
    
    # Domain-specific filtering specifications
    allowed_specs = {
        "civil": ["material_type","coverage","thickness","application_method","standards"],
        "electrical": ["voltage","conductor_material","size_sqmm","insulation_type","standards"],
        "rf cable assembly": ["frequency_range","impedance","connector_type","vswr","insertion_loss"]
    }
    domain_fields = allowed_specs.get(domain_key, [])
    
    # Performance optimization: group SKUs by category once
    sku_by_category = defaultdict(list)
    for sku in catalog:
        sku_by_category[sku.get("category", "")].append(sku)
    
    # ðŸš€ BATCH ENCODING: Precompute all item embeddings at once
    print(f"[Technical Agent] ðŸš€ Batch encoding {len(items)} items...")
    item_texts = [f"{i.get('item_name', '')} {i.get('required_technical_specs', '')}" for i in items]
    with torch.no_grad():
        item_embeddings = MODEL.encode(item_texts, convert_to_tensor=True)
    print(f"[Technical Agent] âœ… Batch encoding complete")
    
    # 4. SEQUENTIAL SKU MATCHING SYSTEM (No parallelization)
    rfp_items_output = []
    
    for idx, item in enumerate(items):
        item_name = item.get("item_name", "Unknown")
        item_specs = item.get("required_technical_specs", "")
        item_text = f"{item_name} {item_specs}"
        
        print(f"\n[Technical Agent] Processing: {item_name}")
        
        # STEP 1: Extract Structured Specifications (but ignore LLM category)
        structured_specs = extract_structured_specs(item_text, item_name)
        
        # STEP 1.5: Use Master Agent item_name as category (never trust LLM)
        category = item_name
        structured_specs["category"] = category
        
        # STEP 1.6: Domain-specific filtering for gap intelligence
        # ---- SAFE FILTER (do NOT wipe specs accidentally) ----
        if domain_fields:
            structured_specs = {
                k: v for k, v in structured_specs.items()
                if k in domain_fields or k == "category"
            }
        
        print(f"[Technical Agent] Using strict category: {category}")
        
        # STEP 2: Fast category lookup from precomputed groups
        candidate_skus = sku_by_category.get(category, [])
        
        # STEP 2.5: Use category-specific candidates directly
        final_candidates = candidate_skus if candidate_skus else get_candidate_skus(structured_specs)
        
        # print(f"[Technical Agent] â†’ Category candidates: {len(final_candidates)} SKUs")  # ðŸš€ OPTIMIZATION 7: removed print in loop
        
        # ðŸš€ STEP 3: Use precomputed batch embedding
        item_embedding = item_embeddings[idx]
        
        # STEP 3.1: Vector Ranking (now on filtered candidates only)
        ranked_candidates = vector_rank_candidates(item_text, final_candidates, top_k=10, item_embedding=item_embedding)
        
        # STEP 4: Rule-Based Scoring & Final Selection
        scored_candidates = []
        for candidate in ranked_candidates:
            vector_sim = candidate.get("vector_similarity", 0.0)
            rule_score = calculate_spec_score(structured_specs, candidate, vector_sim)
            
            candidate_result = {
                "sku_code": candidate.get("sku_code"),
                "price_per_unit": candidate.get("unit_price_inr", 1500),
                "sku_description": candidate.get("product_name", ""),
                "spec_match_percent": rule_score,
                "vector_similarity": vector_sim,
                "category": candidate.get("category", "")
            }
            scored_candidates.append(candidate_result)
        
        # 3) REMOVE DUPLICATE SKU DEFENSIVE CODE - just sort and take top 3
        scored_candidates.sort(key=lambda x: x["spec_match_percent"], reverse=True)
        top_3_skus = scored_candidates[:3]
        
        # STEP 5: Final Match Selection
        if top_3_skus:
            best_match = top_3_skus[0]
            
            # GAP INTELLIGENCE (CLEAN VERSION) - WITH DEBUG
            
            def is_empty(val):
                return val in [None, "", [], {}, "Not Found"]

            sku_doc = CATALOG_MAP.get(best_match["sku_code"], {})

            sku_specs = sku_doc.get("specifications", {})

            missing_specs = []

            for key, rfp_val in structured_specs.items():

                # never compare meta fields
                if key in ["category"]:
                    continue

                if is_empty(rfp_val):
                    continue

                sku_val = sku_specs.get(key)

                # compare only if SKU actually has value
                if is_empty(sku_val):
                    continue

                if str(rfp_val).lower() != str(sku_val).lower():
                    missing_specs.append({
                        "spec_name": key,
                        "rfp_value": rfp_val,
                        "sku_value": sku_val
                    })
                
            # print(f"[GAP DEBUG] Total missing specs found: {len(missing_specs)}")  # ðŸš€ REMOVED: debug print in loop
            
            # Store gap intelligence if specs are unmatched
            if missing_specs:
                # print(f"[GAP DEBUG] Storing {len(missing_specs)} gaps to MongoDB...")  # ðŸš€ REMOVED: debug print in loop
                gap_collection = db["sku_gap_intelligence"]
                try:
                    gap_collection.update_one(
                        {
                            "rfp_id": rfp_id,
                            "item_name": item_name
                        },
                        {
                            "$set": {
                                "best_match_sku": best_match["sku_code"],
                                "spec_match_percent": best_match["spec_match_percent"],
                                "missing_specs": missing_specs,
                                "domain": domain,
                                "category": category,
                                "last_seen": datetime.utcnow()
                            },
                            "$inc": {
                                "occurrences": 1
                            },
                            "$setOnInsert": {
                                "created_at": datetime.utcnow()
                            }
                        },
                        upsert=True
                    )
                    # print(f"[GAP DEBUG] âœ… Successfully stored gap intelligence for {item_name}")  # ðŸš€ REMOVED: debug print in loop
                except Exception as e:
                    print(f"[GAP DEBUG] âŒ Error storing gap intelligence: {e}")
            
            # Determine status based on rule score
            match_score = best_match["spec_match_percent"]
            status = "Matched" if match_score >= 90 else "Warning" if match_score >= 75 else "Review"
            
            rfp_items_output.append({
                "item_name": item_name,
                "required_technical_specs": item_specs,
                "best_match_sku": best_match["sku_code"],
                "spec_match_percent": match_score,
                "status": status,
                "top_3_skus": top_3_skus
            })
            
            # print(f"[Technical Agent] âœ… {item_name} â†’ {best_match['sku_code']} ({match_score:.1f}%) | Category: {category}")  # ðŸš€ OPTIMIZATION 7: removed print in loop
        else:
            # No valid candidates found
            # print(f"[Technical Agent] âŒ No suitable SKUs found for {item_name}")  # ðŸš€ OPTIMIZATION 7: removed print in loop
            rfp_items_output.append({
                "item_name": item_name,
                "required_technical_specs": item_specs,
                "best_match_sku": "NO_MATCH",
                "spec_match_percent": 0.0,
                "status": "No Match",
                "top_3_skus": []
            })

    # 1) PASS DOMAIN TO FRONTEND FORMATTER
    raw_output = {"rfp_id": rfp_id, "items": rfp_items_output, "domain": domain}
    state["technical_output_raw"] = raw_output
    state["technical_output"] = format_technical_for_frontend(raw_output)

    # 6) LIGHTER MEMORY USAGE - cleanup
    del full_text, items
    gc.collect()

    print(f"[Technical Agent] âœ… SKU matching complete for {len(rfp_items_output)} items")
    return state
def pricing_agent_node(state: RFPState) -> RFPState:
    print("\n[Pricing Agent] Started")

    # 1. Validate Technical Input
    technical_output = state.get("technical_output_raw")
    if not technical_output or not technical_output.get("items"):
        print("[Pricing Agent] âŒ No items to price")
        state["pricing_output"] = {"status": "No items"}
        return state

    pricing_summary = state.get("master_output", {}).get("pricing_summary", {})

    # 2. MongoDB: Connect to test_data collection and fetch ALL tests once
    test_data = db["test_data"]
    all_tests = list(test_data.find({}))

    # 3. Pricing Rules
    RISK_THRESHOLD = pricing_summary.get("pricing_rules", {}).get("risk_threshold_percent", 70)

    final_items = []
    total_material = 0
    total_testing = 0

    for item in technical_output["items"]:
        sku = item.get("best_match_sku")
        spec_match = item.get("spec_match_percent", 0)
        item_name = item.get("item_name", "Unknown")

        if not sku or item.get("status") == "Out of Domain" or sku == "NO_MATCH":
            continue

        # Check if top_3_skus is empty (no SKUs found)
        top_3_skus = item.get("top_3_skus", [])
        if not top_3_skus:
            continue

        # --- Material Price (Now Real from DB) ---
        # Pulls the price_per_unit we injected in the Technical Agent (e.g., 890)
        best_sku_data = top_3_skus[0]
        unit_price = best_sku_data.get("price_per_unit", 1500)
        quantity = 1000 
        material_cost = unit_price * quantity

        # --- Testing Cost Calculation (FIXED: Substring matching) ---
        sku_cat = item_name.lower()
        
        matched_tests = []
        for test in all_tests:
            types = [t.lower() for t in test.get("applicable_product_types", [])]
            if any(t in sku_cat or sku_cat in t for t in types):
                matched_tests.append(test)
        
        # Compute testing cost by summing price_inr for matched tests
        testing_cost = sum(t.get("price_inr", 0) for t in matched_tests)
        
        # print(f"[Pricing Agent] Tests matched: {len(matched_tests)} | Testing cost: â‚¹{testing_cost}")  # ðŸš€ OPTIMIZATION 7: removed print in loop

        # 4. Risk Flag & Item Consolidation
        score = spec_match

        if score >= 90:
            risk = "Low"
        elif score >= 75:
            risk = "Medium"
        else:
            risk = "High"

        final_items.append({
            "sku_code": sku,
            "item_name": item_name,
            "quantity": quantity,
            "unit_price": unit_price,
            "material_total": material_cost,
            "testing_cost": testing_cost,
            "line_total": material_cost + testing_cost,
            "risk": risk
        })

        total_material += material_cost
        total_testing += testing_cost

    # 5. Final Output Construction
    state["pricing_output"] = {
        "currency": "INR",
        "items": final_items,
        "material_total": total_material,
        "testing_total": total_testing,
        "grand_total": total_material + total_testing
    }

    # 6. Product Compliance Table
    tech_items = state.get("technical_output_raw", {}).get("items", [])
    pricing_items = state.get("pricing_output", {}).get("items", [])

    # map item_name â†’ price for fast lookup
    price_map = {p["item_name"]: p["unit_price"] for p in pricing_items}

    compliance_rows = []

    for t in tech_items:
        name = t.get("item_name")
        percent = t.get("spec_match_percent", 0)

        if percent >= 80:
            compliance = "Full"
        elif percent >= 60:
            compliance = "Partial"
        else:
            compliance = "Incomplete"

        compliance_rows.append({
            "rfp_item": name,
            "unit_price": price_map.get(name, 0),
            "spec_match_percent": percent,
            "compliance": compliance
        })

    state["product_compliance_table"] = compliance_rows

    print(f"[Pricing Agent] âœ… Pricing complete. Total Testing Cost: {total_testing}")
    return state


# ============================
# PROPOSAL BUILDER
# ============================

def build_bid_prose(technical_output, pricing_output, rfp_context, why_us_text: str = ""):
    items = technical_output.get("items", [])
    pricing_items = pricing_output.get("items", [])

    # --- Project Timeline (Programmatic Dates) ---
    # Default: start = today + 7 days, end = start + 120 days
    from datetime import timedelta as _td

    _today = datetime.now().date()
    _start_date = _today + _td(days=7)
    _end_date = _start_date + _td(days=120)
    start_date_str = _start_date.strftime("%d %b %Y")
    end_date_str = _end_date.strftime("%d %b %Y")

    timeline_text = (
        f"Project Execution Timeline: We hereby commit to a definitive project schedule wherein execution "
        f"shall commence on {start_date_str} (T+7 days from Purchase Order issuance) with assured completion "
        f"by {end_date_str} (within 120 days from mobilization). Our dedicated project team will ensure proactive "
        f"monitoring of critical milestones including Site Mobilization, Material Procurement & Logistics, "
        f"Comprehensive Testing & Quality Assurance, and Final Delivery with documentation handover. "
        f"We are committed to on-time or earlier delivery through systematic project controls and resource allocation."
    )

    # --- Assumptions, Inclusions, Exclusions (Static Sentences) ---
    assumptions = [
        "Buyer provides uninterrupted site access",
        "Single inspection authority",
    ]
    inclusions = [
        "Routine and acceptance testing",
        "Packaging and transit insurance",
    ]
    exclusions = [
        "Installation & commissioning",
        "Taxes not explicitly mentioned in the RFP",
    ]

    assumptions_text = "Assumptions: " + "; ".join(assumptions) + "."
    inclusions_text = "Inclusions: " + "; ".join(inclusions) + "."
    exclusions_text = "Exclusions: " + "; ".join(exclusions) + "."

    # --- Technical Section ---
    tech_paragraphs = []
    for item in items:
        item_name = item.get("item_name", "")
        sku = item.get("best_match_sku") or item.get("recommended_sku") or ""
        if not item_name and not sku:
            continue
        tech_paragraphs.append(
            f"For {item_name} works, we recommend OEM SKU {sku}, which meets the technical "
            f"and quality requirements outlined in the RFP."
        )

    technical_text = " ".join(tech_paragraphs)

    # --- Pricing Section ---
    pricing_lines = []
    for p in pricing_items:
        pricing_lines.append(
            f"The item {p['item_name']} with SKU {p['sku_code']} is priced at "
            f"â‚¹{p['unit_price']} per unit for a quantity of {p['quantity']}. "
            f"The total material cost is â‚¹{p['material_total']:,}, "
            f"with applicable testing charges of â‚¹{p['testing_cost']:,}, "
            f"resulting in a line total of â‚¹{p['line_total']:,}."
        )

    pricing_text = " ".join(pricing_lines)

    # --- Final Bid Prose ---
    material_type_text = ""
    if rfp_context.get("material_type"):
        material_type_text = f"Material Scope: {rfp_context.get('material_type')}."

    prose = f"""
Subject: Proposal for {rfp_context['title']} (RFP ID: {rfp_context['rfp_id']})

We thank {rfp_context['buyer']} for inviting bids for the subject RFP titled 
"{rfp_context['title']}". We have carefully reviewed the tender documents and 
understand the scope of work along with the applicable technical and commercial 
requirements.

{material_type_text}

Based on our technical evaluation, the following solution is proposed:
{technical_text}

Based on the above technical configuration, our commercial offer is as follows:
{pricing_text}

The total bid value amounts to â‚¹{pricing_output['grand_total']:,}. This pricing 
is inclusive of all applicable routine, type, and acceptance testing as required 
by the RFP.

{timeline_text}

{assumptions_text} {inclusions_text} {exclusions_text}

Organizational Credentials & Track Record: Our organization brings strong domain expertise and a proven 
execution track record in the competitive tendering landscape. {why_us_text} This extensive experience 
enables us to deliver reliable, fully compliant, and timely project outcomes consistently for our clients 
across public sector undertakings, state utilities, and private enterprises.

We confirm that the proposed materials comply with relevant standards and can be delivered within the 
timelines specified, backed by our established supply chain infrastructure and quality management systems.

We look forward to the opportunity to work with {rfp_context['buyer']}.

Yours sincerely,  
Authorized Signatory
"""

    return prose.strip()


def proposal_builder_node(state: RFPState) -> RFPState:
    print("\n[Proposal Builder] Started")

    rfp = state["master_output"]["selected_rfp"]

    # ----------------------------
    # RFP NORMALIZATION (SINGLE SOURCE OF TRUTH)
    # ----------------------------
    # Pull master context for domain + expected categories (source of truth)
    _master_ts = (state.get("master_output") or {}).get("technical_summary") or {}
    _domain = ((_master_ts.get("rfp_context") or {}).get("domain") or rfp.get("domain") or "").strip()
    _categories = (_master_ts.get("scope_context") or {}).get("expected_item_categories") or []

    def _derive_material_type(domain: str, categories: list) -> str:
        d = (domain or "").strip().lower()
        cat_lower = [str(c).strip().lower() for c in (categories or []) if c]

        # Category-driven (Civil)
        civil_map = {
            "interior painting": "Paint & Coatings",
            "structure repair": "Repair & Rehabilitation Materials",
            "waterproofing": "Waterproofing Systems",
        }
        for c in cat_lower:
            if c in civil_map:
                return civil_map[c]

        # Domain-driven
        if any(k in d for k in ["electrical", "power", "cables", "cable"]):
            return "Electrical Cables & Accessories"
        if "rf" in d:
            return "RF Cable Assembly"
        if "civil" in d:
            return "Civil & General Materials"

        # Fallback to existing master summary material type if present
        mt = ((_master_ts.get("scope_context") or {}).get("material_type") or "").strip()
        return mt or "General"

    derived_material_type = _derive_material_type(_domain, _categories)

    # ----------------------------
    # ADAPTIVE "WHY US" (Single MongoDB read from past_bids)
    # ----------------------------
    why_us_text = ""
    try:
        from collections import Counter

        past_bids_collection = db["dummybid"]

        past_docs = list(
            past_bids_collection.find(
                {},
                {
                    "_id": 0,
                    "buyer": 1,
                    "domain": 1,
                    "status": 1
                }
            )
        )

        total_bids = len(past_docs)

        won_bids = sum(1 for d in past_docs if str(d.get("status", "")).lower() == "won")
        lost_bids = sum(1 for d in past_docs if str(d.get("status", "")).lower() == "lost")

        buyer_counter = Counter(d.get("buyer") for d in past_docs if d.get("buyer"))
        domain_set = sorted(set(d.get("domain") for d in past_docs if d.get("domain")))

        win_pct = round((won_bids / total_bids) * 100) if total_bids else 0

        top_buyers = ", ".join([b for b, _ in buyer_counter.most_common(3)]) or "multiple PSU & private buyers"
        domains_phrase = ", ".join(domain_set) or "multiple domains"

        why_us_text = (
            f"We have participated in {total_bids} competitive tenders, with {won_bids} successful awards "
            f"and {lost_bids} losses, resulting in a win rate of {win_pct}%. "
            f"We have delivered projects across {domains_phrase} and frequently work with {top_buyers}."
        )

    except Exception as _e:
        why_us_text = (
            "We have participated in 0 competitive tenders recorded in our historical repository; as additional outcomes are captured, this section will automatically reflect our win/loss performance."
        )

    # ----------------------------
    # BUILD FINAL BID PROSE
    # ----------------------------
    rfp_context = {
        "rfp_id": rfp.get("rfp_id"),
        "title": rfp.get("rfp_title"),
        "buyer": rfp.get("buyer"),
        "material_type": derived_material_type,
    }

    final_bid_prose = build_bid_prose(
        technical_output=state["technical_output_raw"],
        pricing_output=state["pricing_output"],
        rfp_context=rfp_context,
        why_us_text=why_us_text,
    )

    state["final_bid_prose"] = final_bid_prose

    print("[Proposal Builder] âœ… Final Proposal Generated")
    return state


# ============================
# LANGGRAPH BUILD
# ============================

# 5) MAKE GRAPH EXECUTION PURE SEQUENTIAL
graph = StateGraph(RFPState)

graph.add_node("sales_agent", sales_agent_node)
graph.add_node("priority_selector", select_highest_priority_rfp)
graph.add_node("technical_agent", technical_agent_node)
graph.add_node("pricing_agent", pricing_agent_node)
graph.add_node("master_agent", master_agent_node)
graph.add_node("proposal", proposal_builder_node)

graph.set_entry_point("sales_agent")

# Sequential flow: sales â†’ priority â†’ master â†’ technical â†’ pricing â†’ proposal â†’ END
graph.add_edge("sales_agent", "priority_selector")
graph.add_edge("priority_selector", "master_agent")
graph.add_edge("master_agent", "technical_agent")
graph.add_edge("technical_agent", "pricing_agent")
graph.add_edge("pricing_agent", "proposal")
graph.add_edge("proposal", END)

rfp_graph = graph.compile()

# ============================
# RUN
# ============================

if __name__ == "__main__":
    print("="*60)
    print("RFP PROCESSING SYSTEM - LOCAL OLLAMA MODE")
    print("="*60)
    
    # Pre-flight check: Verify Ollama is ready
    if not check_ollama_availability():
        print("\n[CRITICAL] Ollama is not available. Exiting.")
        print("\nSetup Instructions:")
        print("1. Install Ollama from https://ollama.ai")
        print("2. Start Ollama: ollama serve")
        print(f"3. Download model: ollama pull {OLLAMA_MODEL}")
        exit(1)
    
    # ðŸš€ OPTIMIZATION 8: Load persisted LLM cache BEFORE workflow
    load_llm_cache_from_disk()
    
    # ðŸš€ OPTIMIZATION 4: Embeddings already preloaded at module level
    # initialize_sku_embeddings()  # Not needed - already done at import
    
    print(f"\n[INFO] Using local model: {OLLAMA_MODEL}")
    print(f"[INFO] Cache enabled: {len(_llm_cache)} cached responses")
    print(f"[INFO] SKU embeddings preloaded: {len(CATALOG)} items")
    print("\n" + "="*60 + "\n")
    
    # 5) PURE SEQUENTIAL EXECUTION
    final_state = rfp_graph.invoke({})
    
    print("\n" + "="*60)
    print("=== FINAL LANGGRAPH STATE ===")
    print("="*60)
    print("\n=== TECHNICAL AGENT OUTPUT (FRONTEND) ===")
    print(json.dumps(
        final_state.get("technical_output", {}),
        indent=2
    ))

    print("\n=== COMPARISON MATRIX (FIRST ITEM) ===")
    tech_raw = final_state.get("technical_output_raw", {})
    if tech_raw.get("items"):
        # 2) PASS DOMAIN WHEN BUILDING MATRIX
        matrix = build_comparison_matrix(
            tech_raw["items"][0],
            domain=tech_raw.get("domain")
        )
        print(json.dumps(matrix, indent=2))

    print("\n=== PRICING AGENT OUTPUT ===")
    print(json.dumps(
        final_state.get("pricing_output", {}),
        indent=2
    ))

    print("\n=== PRODUCT COMPLIANCE TABLE ===")
    print(json.dumps(
        final_state.get("product_compliance_table", []),
        indent=2
    ))

    print("\n" + "="*60)
    print("=== FINAL BID PROPOSAL ===")
    print("="*60)
    final_bid = final_state.get("final_bid_prose", "")
    if final_bid:
        print(final_bid)
    else:
        print("âš ï¸ No bid prose generated")
    
    # ðŸš€ OPTIMIZATION 8: Save LLM cache to disk AFTER workflow
    save_llm_cache_to_disk()
    
    # 6) LIGHTER MEMORY USAGE - final cleanup
    gc.collect()
    print("\n[CLEANUP] âœ… Memory cleanup completed")